---
layout: post
title: Deep Learning Theory Course Review Chapters 1-3
---

I'm currently self studying the Deep Learning Theory course from Professor Matus Telgarsky at University of Illinois Urbana-Champaign. The link to the notes are at:


[https://mjt.cs.illinois.edu/dlt/](https://mjt.cs.illinois.edu/dlt/)

and the link to the rest of the course is at:

[https://mjt.cs.illinois.edu/courses/dlt-f20/](https://mjt.cs.illinois.edu/courses/dlt-f20/)

My purpose with this blog post is to both document my progress in the form of notes and help others who are also teaching themselves with these course notes. I recommend reading this post side by side with the course notes.

I'm also aware that these course notes, as of writing this, are fairly early in development. I still found this to be a good resource for learning deep learning style proofs.

These are simply my notes and there may be errors. If you find any, please let me know through my email on my about page.
<hr>
## Preface
### Risk Decomposition
Overall the prerequisite content described in the preface was pretty familiar to me. Most are definitions like the loss function and non-linearities which should be familiar to people with any machine learning background.

One interesting decomposition provided was:

<img src="/images/risk_decomp.png">

Here, $$R$$ stands for risk which is the expected loss and $$\hat{R}$$ is the empirical risk which is the actual loss on the data. $$\hat{f}$$ is the algorithms prediction for $$f$$ and $$\bar{f}$$ is some baseline function.

This seemingly trivial decomposition (every pair of terms on the LHS starting from the right cancel) is described to be the main approach for the proofs provided throughout the notes so it will be something to look out for.

### Complexity
When the term "complexity" is used in the notes, it doesn't refer to the big O complexity but rather an intuitive notion of complexity that say a large neural network has over a simple linear regression model. Given this, let's try to understand the following remark:

<img src="/images/sensitivity_to_complexity.png">

Again, these are intuitive notions of complexity yet to be formalized. The first point is basically saying that if the complexity of the model is low, it's more likely to generalize (difference on the test set and training set is small). This is commonly known as "overfitting" or "fitting the noise". The second point describes how we are looking for a model which has a similar complexity to the baseline without incurring too much cost to its empirical risk or loss.
## Chapter 1: Approximation (preface)
### Using a baseline and motivating norms of functions
Given that we want a model with low complexity and low risk, we can accomplish it by restricting it to some set of low complexity functions $$\mathcal{F}$$ and take the infimum over the risk that is $$\inf_{f\in \mathcal{F}}R(f)$$.

However, we can retain this information while comparing it with a family of functions which we know can achieve a lower risk (by incurring a higher complexity) by taking the worst case difference in risk. The example provided in the course notes is the set of all continuous functions $$g$$ giving us our final goal of bounding the quantity $$\sup_{\text{g cont.}}\inf_{f\in \mathcal{F}}R(f) - R(g)$$ which will let us assert that given $$f$$ is found in a careful way that can depend on $$g$$, the difference in risk won't exceed some bound.

This is normally a hard problem but if the loss function is $$\rho$$ lipschitz, we can bound it by $$\rho$$ times the norm of the difference of the functions.

### Remark 1.1
This gives a handy discussion about why the uniform norm $$\sup_{x\in S}\lvert f(x) - g(x)\rvert$$ is not used instead in this style of proof. Naturally having to take an integral instead of a supremum is going to be tougher. An intuitive explanation (along with a reference) is given that some of these target functions (ex: Lipschitz cont. functions) increase in complexity exponentially with respect to dimension requiring a more refined norm. It's also given that norms like the $$L^1$$ norm are preferred for lower bound proofs instead because with the uniform norm, only a single point has to be off for the function to be seen as completely different. This is intuitive because the uniform norm is the $$L^\infty$$ norm which is good for upperbounds so for lowerbounds, we go for the $$L^1$$ norm which has less sensitivity.
## Chapter 2: Elementary Constructive Approximations
With this chapter, we get more into mathematics over the intuitions.
### Finite-Width Univariate Approximation
<img src="/images/finite-width-univariate-approximation.png">
This proposition and proof is in a similar vein to a riemann sum except we are approximating the curve as opposed to area and instead of considering a partition of the domain and "bars" that go up for each portion, we have the $$1[x \geq 0]$$ function that is simply 1 if $$x \geq 0$$ and 0 otherwise. To construct a function from left to right using a linear combination of $$1[x - b_i]$$ functions, we need to repeatedly step up or step down the function to the appropriate values. Which is why $$\sum_i a_i 1[x-b_i]$$ has the initialization it does. We start with $$a_1 = g(0)$$ which gets the approximation started on the left with $$g(0)1[x - 0]$$ and then subtracts out $$g(0)$$ at the new location and adds the new value of g also at that location. Namely $$g(0)1[x - 0] - g(0)1[x - b_1] + g(1)1[x - b_1]$$ and so on. Pictures below in the case of cosine.

It turns out that $$m = \lceil \frac{\rho}{\epsilon} \rceil$$ is the exact value we need to bound the absolute difference between $$g$$ and $$f$$ by $$\epsilon$$ in this traditional real analysis style of proof

<img src="/images/univ_approx_diag_1.png" width="300">

<img src="/images/univ_approx_diag_2.png" width="400">

<img src="/images/univ_approx_diag_3.png" width="500">
<img src="/images/univ_approx_diag_4.png" width="400">
### Infinite-Width Univariate Approximation
The infinite width network as the name implies takes the limit as the number of nodes goes to infinity, so we get integrals instead of sums. Converting the idea from the previous section to the infinite case and assuming $$g$$ is differentiable and $$g(0) = 0$$ we get that $$g(x) = \int_0^1 1[x\geq b]g^\prime(b)db$$ by the Fundamental Theorem of Calculus.
### Multivariate Approximation Via Bumps
<img src="/images/thm_2_2.png" width="800">
The key for this theorem is that we are now working with a 3 layer neural network (2 hidden and 1 output layer). Intuitively, this means we can construct a new base function (instead of the unit step function) using the first 2 hidden layers and take a linear combination in the output layer. We have also dropped the $$\rho$$-lipschitz condition on the target function. Here, the infinity norm that we use to evoke the definition of continuity is simply for mathematical ease. Any one of the $$L^p$$ norms could have been chosen because an epsilon ball using any of the $$L^p$$ norms could fit inside any other epsilon ball of a different $$L^p$$ norm if we decrease the radius enough which can be used to show they are equivalent notions of continuity.

The new base function we chose in the multivariate case is an indicator function over rectangles in the domain. Namely a function which is 1 over a rectangle in the domain and 0 everywhere else. Taking a linear combination, we can scale each of these rectangles up to approximate the function locally. Similar to a Riemann sum but instead of the area, we focus on approximating the curve with the top of the rectangle. Hence the lemma:
<img src="/images/lemma_2_1.png" width="800">

Using this lemma, we show that the linear combination of our indicators can get arbitrarily close to the target function. If we can show our 2-layer approximation of each of the indicators can get arbitrarily close to the indicators, then we can show our neural network can approximate the target function with a certain size given in the theorem. The approximation is constructed using the following:
<img src="/images/thm_2_1_pf.png" width="800">
where $$\sigma$$ is just the ReLU activation function.

$$g_{\gamma, j}$$ expresses our one dimensional approximation of our indicator function. This works because if we look at just the first two terms of the expression, we have something like below:

<img src="/images/thm_2_1_pf_diag1.png" width="400">
<img src="/images/thm_2_1_pf_diag2.png" width="400">

Desmos has symbol restrictions so $$\rho$$ and $$\delta$$ are used. As $$\delta$$ decreases, we get the same function but steeper. We see that the next two terms are just the same as the first two except some signs are altered and it is now displaced by $$b_j \geq a_j$$. This gives us a "step down". We get something that looks as follows:

<img src="/images/thm_2_1_pf_diag3.png" width="400">
<img src="/images/thm_2_1_pf_diag4.png" width="400">

This gives us our approximation in one dimension.

Performing $$\sigma(\sum_j g_{\gamma,j}(x_j) - (d - 1))$$ will give us 1 where the sum is d, 0 for when the output of $$\sigma$$ is d - 1 or less (namely when any one of the dimensions is 0) and something between 0 and one elsewhere.

Using this, we approximate the multivariable indicators which then can be combined linearly to produce an approximation for our function.

## Chapter 3: Universal Approximation with a Single Hidden Layer
### 3.1 Multiplication in Shallow Networks
We begin this section by considering the family of functions that can be produced with 2 layer neural networks with arbitrary width on the hidden layer. It is shown that with the cosine activation function, this family is closed under a pointwise product of these functions. This family is also clearly closed under addition because a sum of linear combinations of cosine functions are still a linear combination of a cosine function. An intuition is given of how $$x \mapsto \prod_{i = 1}^d \cos(x_i)^r$$ like terms roughly act like $$x \mapsto \prod_{i = 1}^d 1[x_i\in[a_i, b_i]]$$ which we can displace and scale to form continuous functions over compact sets.

Formally, the definition of a universal approximator is provided below:
<img src="/images/univ_approx_def.png" width="800">
Conditions to guarantee this property are provided with the Stone-Weierstrass theorem:
<img src="/images/stone_weierstrass.png" width="800">

A major emphasis of this chapter is that all of these proofs require that $$\Omega(\frac{1}{\epsilon^d})$$ nodes. Mitigating this seems to be the topic of chapter 4.

### 3.2 Universal Approximation via Stone-Weierstrass
This section is pretty straightforward and proves that the family of functions of 2 layer neural networks with the cosine and exponential activation are universal.
### 3.3 Arbitrary Activations
An easier condition for showing universality is provided along with a sketch proof:
<img src="/images/thm_3_2.png" width="800">
Remark 3.6 also gives some alternative proofs involving universality with citations.
## Summary
These three chapters acted as a build up to the Universal Approximation Theorem. So far the proofs haven't been too challenging and I really appreciate the broad overview of the ideas in the field with references.
